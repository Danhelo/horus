# HORUS: Core Thesis

## The Name

**HORUS** — The falcon-headed god of the sky. The Eye that sees all. In Egyptian mythology, the Eye of Horus represents protection, royal power, and good health. But more than that: it represents _seeing what was hidden_.

We build HORUS to give humans the Eye—to see the soul of ideas, to witness the hidden structure of thought as it moves through a language model's mind.

---

## The One-Liner

**"Paste any text. See its soul."**

---

## What We Believe

### 1. The prompt box is a bottleneck

The dominant interface for AI is a text box. You type words. The model interprets them. You hope it understood. This indirection—language as the only bridge between human intent and model behavior—is lossy, opaque, and limiting.

Prompting is like driving a car by describing where you want to go in prose, hoping the GPS understands. What if you could just _point_?

### 2. Features are the true primitives

Inside language models, there are no words. There are _features_—patterns of activation that represent concepts, relationships, tones, structures. These features are what the model actually "thinks" in. Language is just the I/O layer.

Sparse autoencoders (SAEs) let us extract these features. Anthropic's circuit tracing lets us see how they connect. Neuronpedia makes them accessible via API. The infrastructure exists to work at this level.

**Features are the native language of models. It's time to speak it.**

### 3. Tools are combinatorial explosions

A great tool is not a single capability. It's a _space of possibilities_—a set of primitives that combine in ways the designer didn't anticipate. The spreadsheet. The synthesizer. The web browser. Each enables more than it prescribes.

HORUS must be this kind of tool. Not a fixed workflow, but a **possibility space**. The design must be just right: not too prescriptive (limiting), not too overwhelming (paralyzing). The sweet spot where emergence happens.

### 4. The interface should match the substrate

Image models think in spatial patches. Goodfire's Paint with Ember lets you paint directly on those patches. The interface matches the substrate.

Language models think in sequential feature activations. The interface should be a _traversable space_ of features—a graph you can navigate, a territory you can explore, paths you can trace and steer.

### 5. Flow dissolves the read/write boundary

The magic isn't in "analyzing text" or "generating text" as separate modes. It's in the _fluid transition_ between them. You paste text, see its feature fingerprint light up. You drag a node, steering the model. New text flows. The fingerprint shifts. You respond.

This is not turn-taking. It's a continuous feedback loop. The boundary between observation and creation dissolves. That dissolution _is_ the product.

---

## The Feeling We're Designing For

When someone uses HORUS, they should feel:

- **"I made something I couldn't have made before."** — The tool expanded their creative capacity.
- **"I saw my own ideas in a new way."** — The visualization revealed structure they sensed but couldn't articulate.
- **"I didn't know I could make an LLM do that."** — The control was finer-grained than prompting allowed.

The emotional residue is _gnosis_—the sense of having glimpsed something true about how minds (artificial or otherwise) process meaning.

---

## Why Now

1. **Goodfire proved the paradigm.** Paint with Ember showed that direct feature manipulation creates a new UX category. They did it for images. No one has done it for text.

2. **Neuronpedia made features accessible.** 50M+ features, searchable. Steering APIs. Circuit tracing integration. The research infrastructure is now a platform.

3. **Circuit tracing revealed the paths.** Anthropic's work shows that features connect into circuits—traceable paths from input to output. The graph isn't just a visualization; it's a map of computation.

4. **Artists are hungry.** The AI art community has absorbed image generation. They're looking for what's next. A new medium. A new instrument.

---

## The Long Game

HORUS v1 is a creative instrument for artists—a way to see and sculpt in ideaspace.

But the substrate we're building is more general:

- **Feature-level apps.** Programs that run on features, not prompts. An app that activates "playfulness" isn't limited to any single prompt phrasing.
- **Meta-LLM orchestration.** LLMs that call tools that modify other LLMs' features. Compositional AI systems that operate below the language layer.
- **Multimodal triggers.** Features that respond to sound, gesture, biosignals. The graph as a living interface to cognition.

We start with the Eye. Where it leads, we follow.

---

## The North Star Metric

**Emergence.**

Did people use HORUS in ways we didn't design? Did they discover interactions we didn't anticipate? Did the tool become more than the sum of its features?

If yes, we succeeded. If no, we over-designed or under-designed. Recalibrate.

---

_The eye opens. The hidden becomes visible. Let's build._
