# The Primitive: Neural Surgery

## The Question

What is the atomic unit of interaction in HORUS?

Not "click a button." Not "type a prompt." Something more fundamental—the irreducible gesture from which all other interactions compose.

---

## The Answer: Neural Surgery

The primitive is **the specific operation a user performs on the feature graph**.

This is deliberate framing. Not "editing" (too passive). Not "controlling" (too mechanical). *Surgery*—precise, intentional, consequential manipulation of a living system.

Each surgery is:
- **Targeted** — affects specific features or regions
- **Graded** — has intensity/magnitude
- **Reversible** — can be undone, experimented with
- **Composable** — combines with other surgeries

---

## The Toolkit

Users need a vocabulary of surgical operations. Here's the initial set:

### 1. **ILLUMINATE**
Highlight a feature or region. See what's active. Pure observation—but observation that changes what you notice next.

*Gesture:* Hover, select, focus
*Effect:* Visual emphasis, information surfacing

### 2. **AMPLIFY / ATTENUATE**
Turn a feature up or down. The core steering operation.

*Gesture:* Dial, slider, scroll on selection
*Effect:* Feature strength changes, generation shifts

### 3. **NAVIGATE**
Move through the graph. Change your vantage point in ideaspace.

*Gesture:* Pan, zoom, rotate, teleport
*Effect:* Different features enter/exit view

### 4. **ZOOM (Semantic)**
Change abstraction level. Zoom in = more granular features. Zoom out = higher-level concepts.

*Gesture:* Scroll, pinch
*Effect:* Hierarchy expands/collapses, detail level shifts

### 5. **PIN**
Lock a feature's value. It stays constant while you manipulate others.

*Gesture:* Click to pin, click again to release
*Effect:* Constraint on steering—explore variations with one thing held constant

### 6. **TRACE**
Follow a path through feature space. See the trajectory of a text or a generation.

*Gesture:* Scrub timeline, animate path
*Effect:* Temporal structure revealed

### 7. **BLEND**
Interpolate between two states. Morph from one feature configuration to another.

*Gesture:* Drag between two points, slider between two snapshots
*Effect:* Smooth transition, intermediate states generated

### 8. **SNAPSHOT**
Save the current state. A point in the space you can return to or compare against.

*Gesture:* Save button, named bookmark
*Effect:* State preserved for later

### 9. **DIFF**
Compare two states. What features differ? By how much?

*Gesture:* Select two snapshots, enter diff view
*Effect:* Divergence visualized

### 10. **QUERY** (Advanced)
Ask the LLM to find or construct a region of ideaspace.

*Gesture:* Natural language input
*Effect:* Graph navigates to semantic target, relevant features highlighted

---

## Composability

The power is in combination:

- **ILLUMINATE + AMPLIFY** = "What happens if I boost what's already active?"
- **PIN + NAVIGATE** = "Hold creativity constant, explore different topics"
- **TRACE + DIFF** = "How did this text's journey differ from that one's?"
- **QUERY + ZOOM** = "Take me to 'melancholy', then show me its constituents"

This is the combinatorial explosion. N primitives → N² combinations → emergent interactions we didn't design.

---

## What We Don't Know Yet

### How constrained should combinations be?
- **Open:** Any primitive composes with any other. Maximum emergence, risk of chaos.
- **Constrained:** Certain combinations are privileged or guided. Cleaner, but less discovery.
- **Hybrid:** A "grammar" that suggests valid compositions without forbidding others.

### How do we surface composability?
- Explicit UI showing "you can combine these"?
- Emergent discovery through play?
- Tutorials/examples that demonstrate combinations?

### How does this extend?
- Can users define new primitives?
- Can primitives be shared as "tools" others can use?
- Are there "macros"—saved sequences of operations?

---

## The Meta Question

Eventually, the primitive layer itself becomes programmable:

- **Feature-level apps:** A "tone adjuster" app is just ILLUMINATE + AMPLIFY on tone-related features, packaged with a nice UI.
- **LLM-assisted surgery:** "Make this more playful" → the system translates intent into a sequence of operations.
- **Cross-model surgery:** Operations that affect one model's features trigger changes in another's.

The surgical toolkit is the foundation. What gets built on it is unbounded.

---

*The hand moves. The graph shifts. The text transforms. Neural surgery as creative act.*
